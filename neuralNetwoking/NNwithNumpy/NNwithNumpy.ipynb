{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\maith\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1072341617.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 26\u001b[1;36m\u001b[0m\n\u001b[1;33m    if y == None :\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class ShallowNeuralNet(object):\n",
    "    def __init__(self,input_size,hidden_size,output_size,std =1e-4):\n",
    "        # W1 First layer weights , shape(D,H)\n",
    "        #b1 Lbiases ,shape(H,)\n",
    "        #W2 second layer weights , shape(H,C)\n",
    "        #b2 biases , shape(C,)  \n",
    "\n",
    "        self.param = {}\n",
    "        self.param['w1'] = std*np.random.rand(input_size,hidden_size)\n",
    "        self.param['b1'] = np.random.rand(hidden_size,1)\n",
    "        self.param['W2'] = std*np.random.rand(hidden_size,output_size)\n",
    "        self.param['b2'] = np.random.rand(output_size)\n",
    "\n",
    "    def loss(self,X,y=None,reg=0.0):\n",
    "        W1,b1 = self.param['W1'],self.param['b1']\n",
    "        W2,b2 = self.param['W2'],self.param['b2']\n",
    "\n",
    "        N,D = X.shape()\n",
    "\n",
    "        #FeedForward\n",
    "        Z1 = X.dot(W1) +b1\n",
    "        # relu\n",
    "        A1 = np.maximum(0,Z1) \n",
    "        Z2 = A1.dot(W2) + b2  \n",
    "\n",
    "        if y is  None :\n",
    "            return Z2\n",
    "            \n",
    "        Z2 -= np.max(Z2,axis=1,keepdims=True)\n",
    "        Z2_exp = np.exp(Z2)\n",
    "        scores = np.sum(-np.log(scores[np.arange(N)],y))\n",
    "        loss /= N\n",
    "\n",
    "        # BackPropagation\n",
    "        grads = {}\n",
    "        scores[np.arrang(N),y] -= 1\n",
    "        socres /= N\n",
    "\n",
    "        # W2 gradient \n",
    "        dW2 = A1.T.dot(scores)\n",
    "\n",
    "        # b2 gradient \n",
    "        db2 = scores.sum(axis = 0)\n",
    "\n",
    "        # W1 gradient \n",
    "        dA1 = scores.dot(W2.T)\n",
    "        dZ1 = dA1 *(Z1 > 0)\n",
    "        dW1 = X.T.dot(dZ1)\n",
    "\n",
    "        # b1 gradient \n",
    "        db1 = dZ1.sum(axis = 0)\n",
    "\n",
    "        # regularation gradient \n",
    "        dW1 += reg *2*W1\n",
    "        dW2 += reg *2 * W2\n",
    "\n",
    "        grads = {'W1':dW1,'W2':dW2,'b1':db1,'b2':db2}\n",
    "\n",
    "        return loss , grads\n",
    "        \n",
    "    def fit (self,X,y,X_val,y_val,lr = 1e-3,lr_decay = 0.95,reg = 5e-6,num_iters = 100,batch_size = 200,verbose = True):\n",
    "        num_train = X.shape[0]\n",
    "        iters_per_epoch = max(1,num_train // batch_size)\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        # Mini batch\n",
    "        for iters in range (num_train):\n",
    "            batches = np.random.choice(num_train,batch_size)\n",
    "            X_batch = X[batches]\n",
    "            y_batch = y[batches]\n",
    "\n",
    "            # compute loss and gradient\n",
    "            loss,grad = self.loss(X_batch,y_batch,reg = reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # update parameters \n",
    "            for k in self.params:\n",
    "                self.params[k] -= lr*grads[k]\n",
    "\n",
    "            if verbose and iters %100 == 0:\n",
    "                train_acc = (self.predict(X_batch)== y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "\n",
    "        def predict(self,X):\n",
    "            y_preds = np.argmax(self.loss(X),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
