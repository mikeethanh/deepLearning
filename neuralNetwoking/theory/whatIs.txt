Bài trước học về thuật toán logistic regression với giá trị đầu ra là giá trị nhị phân. Tuy nhiên,
logistic regression là một mô hình neural network đơn giản, bài này sẽ học mô hình neural network
đầy đủ

Con chó có thể phân biệt được người thân trong gia đình và người lạ hay đứa trẻ có thể phân biệt được
các con vật. Những việc tưởng chừng như rất đơn giản nhưng lại cực kì khó để thực hiện bằng máy
tính. Vậy sự khác biệt nằm ở đâu? Câu trả lời nằm ở cấu trúc bộ não với lượng lớn các nơ-ron thần
kinh liên kết với nhau. Liệu máy tính có thể mô phỏng lại cấu trúc bộ não để giải các bài toán trên ???
Neural là tính từ của neuron (nơ-ron), network chỉ cấu trúc, cách các nơ-ron đó liên kết với
nhau, nên neural network (NN) là một hệ thống tính toán lấy cảm hứng từ sự hoạt động của các
nơ-ron trong hệ thần kinh.

5.1.1 Hoạt động của các nơ-ron
Nơ-ron là đơn vị cơ bản cấu tạo hệ thống thần kinh và là thành phần quan trọng nhất của não. Đầu
chúng ta gồm khoảng 10 triệu nơ-ron và mỗi nơ-ron lại liên kết với tầm 10.000 nơ-ron khác.
Ở mỗi nơ-ron có phần thân (soma) chứa nhân, các tín hiệu đầu vào qua sợi nhánh (dendrites)
và các tín hiệu đầu ra qua sợi trục (axon) kết nối với các nơ-ron khác. Hiểu đơn giản mỗi nơ-ron
nhận dữ liệu đầu vào qua sợi nhánh và truyền dữ liệu đầu ra qua sợi trục, đến các sợi nhánh của các
nơ-ron khác.

Mỗi nơ-ron nhận xung điện từ các nơ-ron khác qua sợi nhánh. Nếu các xung điện này đủ lớn
để kích hoạt nơ-ron, thì tín hiệu này đi qua sợi trục đến các sợi nhánh của các nơ-ron khác. => Ở
mỗi nơ-ron cần quyết định có kích hoạt nơ-ron đấy hay không. Tương tự các hoạt động của hàm
sigmoid bài trước.

Tuy nhiên NN chỉ là lấy cảm hứng từ não bộ và cách nó hoạt động, chứ không phải bắt chước toàn
bộ các chức năng của nó. Việc chính của chúng ta là dùng mô hình đấy đi giải quyết các bài toán
chúng ta cần

Hàm sigmoid ở đây được gọi là activation function

5.2.2 Mô hình tổng quát
Layer đầu tiên là input layer, các layer ở giữa được gọi là hidden layer, layer cuối cùng được gọi là
output layer. Các hình tròn được gọi là node.
Mỗi mô hình luôn có 1 input layer, 1 output layer, có thể có hoặc không các hidden layer. Tổng số
layer trong mô hình được quy ước là số layer - 1 (không tính input layer).
Ví dụ như ở hình trên có 1 input layer, 2 hidden layer và 1 output layer. Số lượng layer của
mô hình là 3 layer.
Mỗi node trong hidden layer và output layer :
• Liên kết với tất cả các node ở layer trước đó với các hệ số w riêng.
• Mỗi node có 1 hệ số bias b riêng.
• Diễn ra 2 bước: tính tổng linear và áp dụng activation function.

5.2.2 Mô hình tổng quát
Layer đầu tiên là input layer, các layer ở giữa được gọi là hidden layer, layer cuối cùng được gọi là
output layer. Các hình tròn được gọi là node.
Mỗi mô hình luôn có 1 input layer, 1 output layer, có thể có hoặc không các hidden layer. Tổng số
layer trong mô hình được quy ước là số layer - 1 (không tính input layer).
Ví dụ như ở hình trên có 1 input layer, 2 hidden layer và 1 output layer. Số lượng layer của
mô hình là 3 layer.
Mỗi node trong hidden layer và output layer :
• Liên kết với tất cả các node ở layer trước đó với các hệ số w riêng.
• Mỗi node có 1 hệ số bias b riêng.
• Diễn ra 2 bước: tính tổng linear và áp dụng activation function.

5.3 Feedforward

5.3.1 Biểu diễn dưới dạng ma trận
Tuy nhiên khi làm việc với dữ liệu ta cần tính dự đoán cho nhiều dữ liệu một lúc, nên gọi X là ma
trận n*d, trong đó n là số dữ liệu và d là số trường trong mỗi dữ liệu, trong đó x
[i]
j
là giá trị trường
dữ liệu thứ j của dữ liệu thứ i.

Giờ từ input X ta có thể tính được giá trị dự đoán Yˆ, tuy nhiên việc chính cần làm là đi tìm
hệ số W và b. Có thể nghĩ ngay tới thuật toán gradient descent và việc quan trọng nhất trong thuật
toán gradient descent là đi tìm đạo hàm của các hệ số đối với loss function. Và việc tính đạo hàm
của các hệ số trong neural network được thực hiện bởi thuật toán backpropagation, sẽ được giới
thiệu ở bài sau. Và vì bài này có quá nhiều công thức sợ mọi người rối nên code sẽ được để ở bài
sau

5.4 Logistic regression với toán tử XOR
Rõ ràng là không thể dùng một đường thẳng để phân chia dữ liệu thành 2 miền. Nên khi bạn dùng
gradient descent vào bài toán XOR thì bất kể bạn chạy bước 2 bao nhiêu lần hay chỉnh learning_rate
thế nào thì vẫn không ra được kết quả như mong muốn. Logistic regression như bài trước không thể
giải quyết được vấn đề này, giờ cần một giải pháp mới !!

• node NOT(x1 AND x2) chính là từ hình 5.10, với 3 mũi tên chỉ đến từ 1, x1, x2 với hệ số
w0,w1,w2 tương ứng là 1.5, -1, -1.
• node tính x1 OR x2 là từ hình 5.11
• node trong output layer là phép tính AND từ 2 node của layer trước, giá trị hệ số từ hình 1
mang xuống.
Nhận xét: mô hình logistic regression không giải quyết được bài toán XOR nhưng mô hình mới thì
giải quyết được bài toán XOR. Đâu là sự khác nhau:
• Logistic regression chỉ có input layer và output layer
• Mô hình mới có 1 hidden layer có 2 node ở giữa input layer và output layer.
=> Càng nhiều layer và node thì càng giải quyết được các bài toán phức tạp hơn.


1. (a) Tại sao hàm activation phải non-linear? Điều gì xẩy ra nếu hàm linear activation được
sử dụng?
Linear (Tuyến tính): Một hàm được gọi là tuyến tính nếu biểu diễn của nó là một đường thẳng
Non-linear (Không tuyến tính): Một hàm được gọi là không tuyến tính nếu biểu diễn của nó không 
phải là một đường thẳng

Trong ngữ cảnh của neural networks và machine learning, hàm activation non-linear thường được sử
 dụng để giúp mô hình học được các biểu diễn phức tạp hơn của dữ liệu. Các hàm như sigmoid, tanh, 
 hoặc ReLU là các ví dụ của hàm activation non-linear thường được sử dụng trong các lớp của neural
  networks để tạo nên độ phức tạp và khả năng học của mô hình.

àm activation thường được thiết kế là non-linear trong các mô hình neural network vì tính chất này
giúp mô hình có khả năng học được các biểu diễn phức tạp và non-linear từ dữ liệu. Nếu hàm activation
 là linear, tức là một hàm tuyến tính như hàm identity (f(x) = x), thì mỗi layer trong mạng neural chỉ
  tương đương với một phép biến đổi tuyến tính. Kết quả là, dù có thêm nhiều layer, tổng thể mô hình vẫn
   chỉ là một hàm tuyến tính.

Nếu một mạng neural chỉ chứa các phép biến đổi tuyến tính, thì việc thêm các layer mới sẽ không tăng khả
 năng biểu diễn của mô hình, và mô hình sẽ giống như một mô hình tuyến tính đơn giản. Điều này giảm bớt khả
  năng của mô hình trong việc học các biểu diễn phức tạp và non-linear của dữ liệu, làm mất đi lợi ích của việc
   sử dụng mạng neural.

Hàm activation non-linear, như sigmoid, tanh, hoặc ReLU, giúp mô hình học được các mối quan hệ phức tạp giữa 
các đặc trưng đầu vào và đầu ra, cũng như giúp mô hình khả năng học được các đặc trưng phức tạp của dữ liệu.


 Tại sao cần nhiều layer và nhiều node trong 1 hidden layer?
 
Biểu diễn đa tầng (Hierarchical Representation): Mỗi layer trong một neural network có thể học các đặc trưng
 hoặc mức độ trừu tượng khác nhau. Các layer sâu hơn thường học các đặc trưng cấp cao hơn và phức tạp hơn so 
 với các layer gần input. Bằng cách này, mô hình có khả năng học các biểu diễn theo cách có tổ chức và cấp độ khác nhau.

Khả năng học các biểu diễn phức tạp: Một số vấn đề thực tế có độ phức tạp cao, và để có khả năng mô hình hóa
 chúng, cần có đủ độ phức tạp trong kiến trúc của mạng neural. Các hidden layer với nhiều node cung cấp khả
  năng biểu diễn đa dạng hơn của các hàm số, giúp mô hình học được các biểu diễn phức tạp.

Tính phi tuyến (Non-linearity): Nếu chỉ sử dụng một layer và ít node, mô hình sẽ giống như một mô hình tuyến
 tính và sẽ không có khả năng học các mối quan hệ phi tuyến tính giữa các đặc trưng. Việc thêm các layer và
  node với các hàm activation phi tuyến giúp mô hình trở nên phi tuyến tính, mở rộng khả năng biểu diễn của nó.

Regularization và tránh overfitting: Việc sử dụng nhiều layer và node cũng có thể giúp kiểm soát việc overfitting
 (quá mức học) bằng cách áp dụng regularization tự nhiên. Một số phương pháp như dropout và weight regularization
  có thể được áp dụng để giảm thiểu rủi ro overfitting.